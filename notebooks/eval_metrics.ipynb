{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))  \n",
    "if module_path not in sys.path:       \n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import museval\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import IPython.display as ipd\n",
    "from utils.helper import wav_to_spectrogram_clips, rebuild_audio_from_spectro_clips\n",
    "from utils.dataset import create_samples\n",
    "from models.conv_denoising_unet import ConvDenoisingUnet\n",
    "from models.conv_encoder_denoising_decoder import ConvEncoderDenoisingDecoder\n",
    "from models.conv_resblock_denoising_unet import ConvResblockDenoisingUnet\n",
    "from evaluation import evaluate\n",
    "#from evaluation.evaluate import get_separated_tracks, get_reference_tracks, estimate_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv1d_DAE?time=2020-02-24_04:35.h5',\n",
       " 'conv1d_DAE?time=2020-02-26_11:55.h5',\n",
       " 'conv_denoising_unet?time=20200223_0347.h5',\n",
       " 'conv_denoising_unet?time=20200223_1031_with_sum_constraint.h5',\n",
       " 'conv_denoising_unet?time=20200226_1546_with_sum_constraint.h5',\n",
       " 'conv_encoder_denoising_decoder?time=20200224_0618.h5',\n",
       " 'conv_encoder_denoising_decoder?time=20200224_0738.h5',\n",
       " 'conv_encoder_denoising_decoder?time=20200227_0838_l2_weight_regularization.h5',\n",
       " 'conv_res56_denoising_unet?time=20200227_0646_l2_reg.h5',\n",
       " 'conv_resblock_denoising_unet?time=20200229_1806_l1_reg.h5',\n",
       " 'conv_resblock_denoising_unet?time=20200301_1113.h5',\n",
       " 'weight_checkpoints']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(os.listdir(os.path.join(os.pardir, 'saved_model')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '021 - James May - On The Line', 'mix': '/media/yossarian42/master_thesis/audio_source_separator/data/DSD100/Mixtures/Test/021 - James May - On The Line/mixture.wav', 'vocals': '/media/yossarian42/master_thesis/audio_source_separator/data/DSD100/Sources/Test/021 - James May - On The Line/vocals.wav', 'bass': '/media/yossarian42/master_thesis/audio_source_separator/data/DSD100/Sources/Test/021 - James May - On The Line/bass.wav', 'drums': '/media/yossarian42/master_thesis/audio_source_separator/data/DSD100/Sources/Test/021 - James May - On The Line/drums.wav', 'other': '/media/yossarian42/master_thesis/audio_source_separator/data/DSD100/Sources/Test/021 - James May - On The Line/other.wav'}\n"
     ]
    }
   ],
   "source": [
    "samples = create_samples('Test')\n",
    "test_sample = samples[20]\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(os.pardir, 'saved_model', 'conv_denoising_unet?time=20200223_0347.h5')\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-60726d54d29b>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-60726d54d29b>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    separated_sepctrograms = separator.predict(stft_clips)\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_separated_tracks(separator, mix_audio):\n",
    "    # load mix music audio, average the stereo recording to single channel audio track\n",
    "    # convert to spectrogram\n",
    "    sound, sr = librosa.load(mix_audio, sr=44100, mono=True, duration=10)\n",
    "    stft = librosa.stft(sound, n_fft=2048, hop_length=512, win_length=2048)\n",
    "    mag, phase = librosa.magphase(stft)\n",
    "    # chop magnitude of spectrogram into clips, each has 1025 bins, 100 frames\n",
    "    stft_clips = np.empty((0, 1025, 100))\n",
    "    for i in range(mag.shape[1] // 100):\n",
    "        stft_clips = np.concatenate((stft_clips, mag[np.newaxis, :, i * 100: (i + 1) * 100])\n",
    "    # separate components from the mix single channel music audio\n",
    "    separated_sepctrograms = separator.predict(stft_clips)\n",
    "    separated_tracks = list()\n",
    "    # separated_spectrograms contains 4 stem tracks\n",
    "    # the index of spectrograms: 0, 1, 2, 3 -> vocals, bass, drums, other\n",
    "    for i in range(4):\n",
    "        separated_track = np.squeeze(separated_spectrograms[i], axis=-1)\n",
    "        separated_tracks.append(rebuild_audio_from_spectro_clips(separated_track))\n",
    "    return separated_tracks\n",
    "\n",
    "\n",
    "def get_reference_tracks(sample, track_shape):\n",
    "    reference_tracks = list()\n",
    "    for feat in ['vocals', 'bass', 'drums', 'other']:\n",
    "        track, sr = librosa.load(sample[feat], sr=44100, mono=True, duration=10)\n",
    "        # crop reference track to match separated track shape\n",
    "        track = track[tuple(map(slice, track_shape))]\n",
    "        reference_tracks.append(track)\n",
    "    return reference_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_tracks = get_separated_tracks(model, test_sample['mix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_tracks = get_reference_tracks(test_sample['mix'], separate_tracks[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mir_eval\n",
    "\n",
    "(sdr, sir, sar, perm) = mir_eval.separation.bss_eval_sources(references, estimates, compute_permutation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wav_to_spectrogram_clips will remove some frames from the original spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reconstructon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_clips = wav_to_spectrogram_clips(test_sample['mix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spectrogram_clips.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = np.concatenate(spectrogram_clips, axis=1)\n",
    "print(spectrogram_clips.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = rebuild_audio_from_spectro_clips(spectrogram_clips)\n",
    "print('reconstructed audio waveform from wav_to_spectrogram_clips', audio.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound, sr = librosa.load(test_sample['mix'], sr=44100, mono=True)\n",
    "stft = librosa.stft(sound, n_fft=2048, hop_length=512, win_length=2048)\n",
    "mag, phase = librosa.magphase(stft)\n",
    "print(mag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track, sr = librosa.load(test_sample['mix'], sr=44100, mono=True)\n",
    "print('true size of the original audio waveform', track.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
